{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from scipy.special import digamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "\n",
    "def normalize_columns(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the columns of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "    col_sums = input_matrix.sum(axis=0)\n",
    "    new_matrix = input_matrix / col_sums[np.newaxis :]\n",
    "    return new_matrix\n",
    "\n",
    "def normalize_vector(input_vector):\n",
    "    return input_vector/input_vector.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(df, N=0):\n",
    "    \"\"\"\n",
    "    Return documents list, ratings list, and number of documents\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    ratings = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        documents.append(row['review_words'])\n",
    "        ratings.append(float(row['rating']))\n",
    "#         ratings.append((float(row['rating']) - 1.0) / 5.0) # Normalize the ratings form 1-5 to 0-1\n",
    "\n",
    "    # here for testing purposes\n",
    "    if N>0:\n",
    "        documents = documents[:N]\n",
    "        ratings = ratings[:N]\n",
    "        number_of_documents = N\n",
    "    else:\n",
    "        number_of_documents = len(documents)\n",
    "#         max_doc_length = max(self.max_doc_length, len(row['review_words']))\n",
    "    return documents, ratings, number_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(documents, size_V=0):\n",
    "    \"\"\"\n",
    "    Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "    for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "    Update self.vocabulary_size\n",
    "    \"\"\"\n",
    "    words = set()\n",
    "    for line in documents:\n",
    "        words.update(line)\n",
    "    vocabulary = sorted(words)\n",
    "\n",
    "    if size_V > 0:\n",
    "        vocabulary = vocabulary[:size_V]\n",
    "        vocabulary_size = size_V\n",
    "    else:\n",
    "        vocabulary_size = len(vocabulary)\n",
    "\n",
    "    return vocabulary, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_doc_matrix(documents, number_of_documents, vocabulary, vocabulary_size):\n",
    "    \"\"\"\n",
    "    Construct the term-document matrix where each row represents a document, \n",
    "    and each column represents a vocabulary term.\n",
    "\n",
    "    self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "    \"\"\"\n",
    "\n",
    "    idx = dict(zip(vocabulary, range(vocabulary_size)))\n",
    "    # print(idx)\n",
    "    term_doc_matrix = np.zeros([number_of_documents, vocabulary_size], dtype=np.float)\n",
    "    for i, document in enumerate(documents):\n",
    "        for word in document:\n",
    "            term_doc_matrix[i][idx[word]] += 1\n",
    "    # print(self.term_doc_matrix)\n",
    "    \n",
    "    return term_doc_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of aspects (k)\n",
    "number_of_aspects = 5\n",
    "\n",
    "# N - number of documents\n",
    "N = 5\n",
    "\n",
    "# size_V - vocabulary size\n",
    "size_V = 5\n",
    "\n",
    "# doc_df - dataframe of docs\n",
    "doc_df = pd.read_pickle('processed_amazon_reviews.pkl')\n",
    "high_ratings_df = doc_df[doc_df.rating > '3.0']\n",
    "low_ratings_df = doc_df[doc_df.rating < '4.0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build document list and ratings list and get number of documents\n",
    "documents, ratings, number_of_documents = build_corpus(high_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary and get vocabulary size\n",
    "vocabulary, vocabulary_size = build_vocabulary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build term doc matrix\n",
    "term_doc_matrix = build_term_doc_matrix(documents, number_of_documents, vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11533, 8628)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Corpus Level Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = normalize_columns(np.random.rand(self.vocabulary_size, number_of_aspects))\n",
    "s = np.zeros([self.number_of_documents, number_of_aspects])\n",
    "alpha = normalize_rows(np.random.rand(self.number_of_documents, number_of_aspects))\n",
    "\n",
    "z = np.random.randint(number_of_aspects, size=[self.number_of_documents, self.max_doc_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma - distribution of aspects in whole corpus\n",
    "gamma = normalize_vector(np.random.rand(number_of_aspects))\n",
    "beta = np.random.uniform(-1, 1, (number_of_aspects, vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Document Level Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
